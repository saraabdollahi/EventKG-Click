{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EventKG+Click dataset\n",
    "\n",
    "\n",
    "\n",
    "This is a step by step walkthrough to the creation of EventKG+Click dataset which aims to facilitate the creation and evaluation of multilingual user interaction models and reflects the language-specific relevance of events and their relations.  Our dataset EventKG+Click is based on two data sources:\n",
    "* The Wikipedia clickstream2 that refleects real-world user interactions withevents and their relations within language-specific Wikipedia editions; and \n",
    "* The EventKG knowledge graph that contains semantic information regarding events and their relations that partially originates from Wikipedia. EventKG+Click is available online3 to enable further analyses and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset preparation\n",
    "\n",
    "Link to clickstream dataset: \n",
    "https://dumps.wikimedia.org/other/clickstream/\n",
    "    \n",
    "As EventKG+Click and our analysis are based on Wikipedia click behaviour, we only consider those (source, target) click pairs in the clickstream where both the source and target are Wikipedia articles connected by a hyperlink. In our dataset, we adopter Wikipedia clickstream that covers the period from December 1, 2019, to December 31, 2019 and in three language versions, **English**, **German** and **Russian**.\n",
    "\n",
    "The following example shows how we get information from english version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, POST\n",
    "\n",
    "English = pd.read_csv(\"C:/Users/Admin/Downloads/clickstream-enwiki-2019-12.tsv\", sep='\\t',header=None,  error_bad_lines=False) \n",
    "English.columns = ['source','target','link_type','count']\n",
    "English=English.loc[English['link_type'] == 'link'] ### we only choose those that are in the wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "wds = \"http://eventkginterface.l3s.uni-hannover.de/sparql\"\n",
    "\n",
    "entity_mapping_rq='''\n",
    "\n",
    "SELECT substr(str(?entity), 45)\n",
    "{{\n",
    "?entity owl:sameAs {0}\n",
    "}}\n",
    "'''\n",
    "\n",
    "comention_rq='''\n",
    "SELECT sum(?a) as ?cnt WHERE\n",
    "{{\n",
    "?relation2 rdf:subject eventKG-r:{0} .\n",
    "?relation2 rdf:object eventKG-r:{1} .\n",
    "?relation2 eventKG-s:mentions ?a. \n",
    "}}\n",
    "'''\n",
    "\n",
    "\n",
    "event_location_rq= '''\n",
    "SELECT DISTINCT \"{0}\" AS ?event ?location\n",
    "WHERE {{\n",
    "eventKG-r:{0} sem:hasPlace ?locationEventKG.\n",
    "?locationEventKG so:containedInPlace ?co .\n",
    "?co rdfs:label ?location .\n",
    "?co rdf:type dbo:Country .\n",
    "FILTER (LANG(?location)=\"en\")\n",
    "}}\n",
    "'''\n",
    "\n",
    "event_time_rq ='''\n",
    "SELECT distinct \"{0}\" as ?event ?start\n",
    "WHERE\n",
    "{{\n",
    "eventKG-r:{0}  sem:hasBeginTimeStamp ?start .\n",
    "}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparql_request(service, query):\n",
    "    \n",
    "    sparql = SPARQLWrapper(service)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    result = sparql.query()\n",
    "    processed_results = json.load(result.response)\n",
    "    cols = processed_results['head']['vars']\n",
    "    out = []\n",
    "    for row in processed_results['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "        out.append(item)\n",
    "    return pd.DataFrame(out, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity_mapping function\n",
    "\n",
    "In order to leverage the rich information provided in EventKG and find interlinked wikipedia pages in different languages, we need to map title of Wikipedia pages to EventKG. \n",
    "\n",
    "*input: label of wikipedia pages*\n",
    "*output: entity_id in EventKG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_mapping (label):\n",
    "    #options for language: {\"en\",\"de\",\"ru\"}\n",
    "    dbpedia_page=\"<http://dbpedia.org/resource/\"+label+\">\"\n",
    "    temp=get_sparql_dataframe(wds, entity_mapping_rq.format(dbpedia_page))\n",
    "    return temp.iloc[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show how we have collected data using functions, we work on a small dataset which contains all clicked wikipedia \n",
    "pages after exploring **\"World War II\"**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data=pd.read_csv(\"World_War_II_clickstream.txt\", sep='\\t',error_bad_lines=False)\n",
    "en_entity=list(en_data[\"source\"].unique())+list(en_data[\"target\"].unique())\n",
    "mapped_labels = pd.DataFrame(columns=('label','ekg_entity'))\n",
    "\n",
    "for t in range(len(en_entity)):\n",
    "    try:\n",
    "        temp=entity_mapping(str(en_entity[t]),\"\")\n",
    "        mapped_labels=mapped_labels.append({'label' : en_entity[t] , 'ekg_entity':temp} , ignore_index=True)\n",
    "    except:\n",
    "        print(\"The corresponding entity doesn't exist on EventKG\")\n",
    "        \n",
    "en_mapped=pd.merge(left=en_data, right=en_mapped, how=\"left\", left_on=\"target\", right_on=\"label\")\n",
    "en_mapped=en_mapped.rename(columns={\"ekg_entity\":\"target_ekg\"})\n",
    "en_mapped=pd.merge(left=en_data, right=en_mapped, how=\"left\", left_on=\"source\", right_on=\"label\")\n",
    "en_mapped=en_mapped.rename(columns={\"ekg_entity\":\"source_ekg\"})\n",
    "en_mapped=en_mapped[[\"source\",\"target\",\"source_ekg\",\"target_ekg\",\"count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data=pd.read_csv(\"World_War_II_clickstream\", sep='\\t',error_bad_lines=False)\n",
    "en_entity=list(en_data[\"source\"].unique())+list(en_data[\"target\"].unique())\n",
    "mapped_labels = pd.DataFrame(columns=('label','ekg_entity'))\n",
    "\n",
    "for t in range(len(en_entity)):\n",
    "    try:\n",
    "        temp=entity_mapping(str(en_entity[t]))\n",
    "        mapped_labels=mapped_labels.append({'label' : en_entity[t] , 'ekg_entity':temp} , ignore_index=True)\n",
    "        #print(i)\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "en_mapped=pd.merge(left=en_data, right=mapped_labels, how=\"left\", left_on=\"target\", right_on=\"label\")\n",
    "en_mapped=en_mapped.rename(columns={\"ekg_entity\":\"target_ekg\"})\n",
    "en_mapped=pd.merge(left=en_mapped, right=mapped_labels, how=\"left\", left_on=\"source\", right_on=\"label\")\n",
    "en_mapped=en_mapped.rename(columns={\"ekg_entity\":\"source_ekg\"})\n",
    "en_mapped=en_mapped[[\"source\",\"target\",\"source_ekg\",\"target_ekg\",\"count\"]]\n",
    "en_mapped=en_mapped.dropna() ### because we are intereseted only on mapped entities and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining three dataframes\n",
    "\n",
    "After creating **ge_mapped** and **ru_mapped** corresponding mapped wikipedia pages to the knowledge graph, we join three dataframe to get the intersection of events. We are interested to compare relevance of events and relations with respect to the three languages. \n",
    "And since we are intereseted to analyse the events, we only keep targets which are events. to do so, we could easily use the prefixes of entities in Eventkg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_table=pd.merge(pd.merge(de_mapped,en_mapped,how=\"inner\", on=['source_ekg', 'target_ekg']),ru_mapped,how=\"inner\",on=['source_ekg', 'target_ekg'])\n",
    "intersection_table=intersection_table.loc[en_mapped[\"target_ekg\"].str.startswith(\"event\"),]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing data\n",
    "\n",
    "Wikipedia language versions are different in terms of their size, number of user and the amount of edited content. in order to balance the effects of size in each language versions, we normalize the number of clicks with respect to the total\n",
    "number of clicks in the respective language, which leads to normalized scores in the range [0; 1]. In order to create balanced click counts, we then multiply the normalised score by the total number of clicks in the clickstreams.\n",
    "\n",
    "$balanced\\_clicks(e_s,e_t,l) = clicks(e_s,e_t,l) \\cdot \\frac{\\sum_{l' \\in L}\\sum_{e_s' \\in E}\\sum_{e_t' \\in E} clicks(e_s',e_t',l')}{\\sum_{e_s' \\in E}\\sum_{e_t' \\in E} clicks(e_s',e_t',l)} $\n",
    "\n",
    "Using the above formula, normalized scores for each language are as follows which we use directly on the **en_data** dataframe.\n",
    "\n",
    "\n",
    "* English normalized score: 1.6\n",
    "* German normalized score: 4.1\n",
    "* Russian normalized score: 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_table[\"balanced_en_count\"]= 1.6 * intersection_table[\"en_count\"]\n",
    "intersection_table[\"balanced_de_count\"]= 4.1 * intersection_table[\"de_count\"]\n",
    "intersection_table[\"balanced_ru_count\"]= 6.2 * intersection_table[\"ru_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language-specific Relation Relevance\n",
    "\n",
    "This score assigns a relevance score to the relation between a source entity and a target event et in a given language.\n",
    "\n",
    "$relation\\_relevance(e_s,e_t,l) = \\frac{balanced\\_clicks(e_s,e_t,l)}{\\sum_{l' \\in L} balanced\\_clicks(e_s,e_t,l')} \\in [0,1]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "intersection_table['en_normalized'] = intersection_table[\"en_count\"]/(intersection_table[\"ru_count\"]+intersection_table[\"de_count\"]+intersection_table[\"en_count\"])\n",
    "intersection_table['de_normalized'] = intersection_table[\"de_count\"]/(intersection_table[\"ru_count\"]+intersection_table[\"de_count\"]+intersection_table[\"en_count\"])\n",
    "intersection_table['ru_normalized'] = intersection_table[\"ru_count\"]/(intersection_table[\"ru_count\"]+intersection_table[\"de_count\"]+intersection_table[\"en_count\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Location Closeness\n",
    "    \n",
    "Using the following function, we aim to get a set of binary influence factors that indicate whether an event happened in a location where the respective language (*Enlish, German, Russian*) is an offcial language. To do so, we have created **country_language** dataframe that contains countries where English, German, Russian are official languages.\n",
    "\n",
    "*input: a list of events*\n",
    "*output: event and 3 binary columns for English, German and Russian*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#input: list of events\n",
    "#output: a dataframe with 3 columns which shows whether the event has happend in a english, german and russian, speaking location\n",
    "def get_location(events):\n",
    "    events_location=pd.DataFrame()\n",
    "    country_language=pd.read_pickle(\"country_language\")\n",
    "    events=list(en_mapped[\"target_ekg\"].unique())\n",
    "    for i in range (len(events)):\n",
    "        temp=get_sparql_dataframe(wds, event_location_rq.format(events[i]))\n",
    "        events_location=events_location.append(temp)\n",
    "    events_language=pd.merge(left=events_location,right=country_language,how=\"left\", left_on=\"location\", right_on=\"country\")\n",
    "    events_language=events_language.loc[events_language[\"language\"].notna(),]\n",
    "    events_language['english'] = [1 if x =='English' else 0 for x in events_language['language']] \n",
    "    events_language['german'] = [1 if x =='German' else 0 for x in events_language['language']] \n",
    "    events_language['russian'] = [1 if x =='Russian' else 0 for x in events_language['language']] \n",
    "    events_language=events_language[[\"event\",\"english\",\"german\",\"russian\"]]\n",
    "    events_language=events_language.drop_duplicates()\n",
    "       \n",
    "    return events_language\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Recency\n",
    "\n",
    "To observe the impact of recency on the language-specific user click behaviour and using **get_recency** function we compute a recency score which is the number of days between the event start date and the start date of the clickstream dataset which is (2019-12-01)\n",
    "\n",
    "*input: events list*\n",
    "*output: dataframe of two columns: events, receny*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recency():\n",
    "   \n",
    "    events_time=pd.DataFrame()\n",
    "    for i in range (len(events)):\n",
    "        temp=get_sparql_dataframe(wds, event_time_rq.format(events[i]))\n",
    "        events_time=events_time.append(temp)\n",
    "        \n",
    "        \n",
    "events_time[\"max_start_time\"]=events_time.groupby([\"event\"])['start'].transform('max')\n",
    "events_time['max_start_time'] = pd.to_datetime(events_time['max_start_time'], errors='coerce')\n",
    "events_time[\"recency\"]=events_time.apply(lambda row: pd.to_datetime(\"2019-12-1\")-row.max_start_time, axis=1)\n",
    "events_time[\"recency\"]=events_time[\"recency\"].dt.days\n",
    "events_time=events_time[[\"event\",\"recency\"]]\n",
    "events_time=events_time.drop_duplicates()\n",
    "\n",
    "### since there might be more than one time for an event, therefore we use the most recent one\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Community Relevance - number of links to a wikipedia page\n",
    "\n",
    "We use a dump and count the number of incoming links to wikipedia pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links=pd.read_csv(\"worldwar_links.txt\", sep=\"\\t\", error_bad_lines=False)\n",
    "events_link=pd.merge(left=en_mapped, right=links, how=\"left\", left_on=[\"target\"], right_on=[\"page\"])\n",
    "del events_link[\"page\"]\n",
    "events_link[\"links\"]=events_link[\"links\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Community Relevance - number of comentions\n",
    "\n",
    "*input: mappend_data dataframe which contains source and target entities*\n",
    "*output: number of comentions in whole wikipedia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## mentions\n",
    "\n",
    "#input: source and target entities\n",
    "#output: number of their comentions in whole wikipedia\n",
    "\n",
    "def get_comentions(df):\n",
    "    comention=pd.DataFrame(columns={\"source_ekg\",\"target_ekg\",\"comentions\"})\n",
    "    for i in range(df.shape[0]):\n",
    "        try:\n",
    "            temp=get_sparql_dataframe(wds, comention_rq.format(df.iloc[i][\"source_ekg\"], df.iloc[i][\"target_ekg\"]))\n",
    "            comention = comention.append({\"source_ekg\":df.iloc[i][\"source_ekg\"], \"target_ekg\":df.iloc[i][\"target_ekg\"], \"comentions\":temp.loc[0,\"cnt\"]}, ignore_index=True)    \n",
    "        except:\n",
    "            continue\n",
    "    return (comention)\n",
    "#we use EventKG for that\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final table correlation\n",
    "#en_mapped\n",
    "#en_comention\n",
    "#events_link\n",
    "#events_time\n",
    "#events_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations with Influence Factors\n",
    "\n",
    "Given EventKG+Click and the influence factors, we now investigate the correlations between such influence factors and the language-specific relevance scores.To this end, we compute the Pearson correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_merge=pd.merge(pd.merge(pd.merge(pd.merge(left=en_mapped, right=en_comention, how=\"left\", left_on=[\"source_ekg\",\"target_ekg\"], right_on=[\"source_ekg\",\"target_ekg\"]),events_link,how=\"left\",on=['target_ekg']),events_time, how=\"left\", left_on=[\"target_ekg\"], right_on=[\"event\"]), events_language, how=\"left\", left_on=[\"target_ekg\"], right_on=[\"event\"])\n",
    "en_merge=en_merge[[\"source_x\",\"target_x\",\"source_ekg_x\",\"target_ekg\",\"count_x\",\"comentions\",\"links\",\"recency\",\"english\",\"german\",\"russian\"]]\n",
    "en_merge=en_merge.rename(columns={\"source_x\":\"en_source\",\"target_x\":\"en_target\",\"source_ekg_x\":\"source_ekg\",\"count_x\":\"en_count\"})\n",
    "\n",
    "en_merge[\"english\"]=en_merge[\"english\"].fillna(0)\n",
    "en_merge[\"german\"]=en_merge[\"german\"].fillna(0)\n",
    "en_merge[\"russian\"]=en_merge[\"russian\"].fillna(0)\n",
    "en_merge[\"comentions\"]=en_merge[\"comentions\"].fillna(0)\n",
    "en_merge[\"links\"]=en_merge[\"links\"].fillna(0)\n",
    "en_merge[\"recency\"]=en_merge[\"recency\"].fillna(-1)\n",
    "\n",
    "\n",
    "en_merge.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
